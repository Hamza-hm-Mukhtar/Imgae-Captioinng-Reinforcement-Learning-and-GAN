{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlirezaAbedinii/ImageCaptioning/blob/main/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JympNSwZMSO5",
        "outputId": "11cf708f-6982-452f-e278-8622ea8dcc13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP1xrvLjjBtK",
        "outputId": "c7dc0946-fa38-4525-9429-ba95f4c48e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/salaniz/pycocoevalcap.git\n",
            "  Cloning https://github.com/salaniz/pycocoevalcap.git to /tmp/pip-req-build-lb5f7ac_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap.git /tmp/pip-req-build-lb5f7ac_\n",
            "  Resolved https://github.com/salaniz/pycocoevalcap.git to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from pycocoevalcap==1.2) (2.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"git+https://github.com/salaniz/pycocoevalcap.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J2Glooq9R6j",
        "outputId": "1954e61e-e303-48fe-8074-cc11f8c5ce30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bleu in /usr/local/lib/python3.8/dist-packages (0.3)\n",
            "Requirement already satisfied: efficiency in /usr/local/lib/python3.8/dist-packages (from bleu) (1.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (from efficiency->bleu) (3.4.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (3.0.12)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (57.4.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (0.7.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (1.10.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (2.4.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (8.1.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (0.10.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (2.25.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (2.0.7)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (6.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy->efficiency->bleu) (1.0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy->efficiency->bleu) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->efficiency->bleu) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->efficiency->bleu) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy->efficiency->bleu) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy->efficiency->bleu) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6pGO5drS0nm"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2ylaLayPhUa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "import zipfile\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "import re\n",
        "import tqdm\n",
        "import pickle\n",
        "from bleu import list_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.util import ngrams\n",
        "from nltk.translate.bleu_score import SmoothingFunction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXTRzUoOOP4S"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import json\n",
        "import time\n",
        "import itertools\n",
        "import os, os.path\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlHs-uAMSv2H"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from pycocotools.coco import COCO\n",
        "from pycocoevalcap.eval import COCOEvalCap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pylab\n",
        "import random\n",
        "from shutil import copyfile\n",
        "import cv2\n",
        "import time\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJLiys8j1EcQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPool2D, LSTM, add\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Embedding\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9xgFYP8E2V2"
      },
      "outputs": [],
      "source": [
        "from gym import Env\n",
        "from gym import spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p17Rodu9aFr",
        "outputId": "11d9f021-e5c6-423f-a0f8-99acbf804fae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6342834010525136"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "tref = ['a man climbing a mountain',\n",
        " 'a man climbs a mountain',\n",
        " 'a man is climbing a side of a mountain',\n",
        " 'a shirtless man climbs up a steep mountain',\n",
        " 'a young white man is climbing a mountain with a rope as a guide']\n",
        "\n",
        "hyp = 'a man climbs a cliff face'\n",
        "sentence_bleu(tref, hyp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ozabw86XsYg"
      },
      "source": [
        "# Globals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKKcBePdmpR3"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pZuJIoUYKsd"
      },
      "outputs": [],
      "source": [
        "dataType='val2014'\n",
        "\n",
        "\n",
        "dataDir = '/content/drive/MyDrive/Lab_work/Video_Caption/training_caption/ImageCaptioning-main/dataset_MSCOCO'\n",
        "pictures_dir_path = dataDir + '/train2014'\n",
        "project_path = '/content/drive/MyDrive/Lab_work/Video_Caption/training_caption/ImageCaptioning-main/'\n",
        "\n",
        "train_dir_path = dataDir + '/train2014'\n",
        "test_dir_path = dataDir + '/val2014'\n",
        "\n",
        "\n",
        "global_images_ids = []\n",
        "global_data = []\n",
        "global_test_data = []\n",
        "global_train_data = []\n",
        "train_pics = []\n",
        "test_pics = []\n",
        "dataset = pd.DataFrame(columns = ['id', 'is_training' 'r', 'g', 'b',\n",
        "                                  'captions', 'url'])\n",
        "train_df_path = os.path.join(project_path,'train_data_coco.csv')\n",
        "test_df_path = os.path.join(project_path,'test_data_coco.csv')\n",
        "\n",
        "embedding_dim = 2048\n",
        "max_length = 16\n",
        "num_words = 2075\n",
        "train_caps = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWc9-rdliw34",
        "outputId": "dcc7d801-6b7d-4913-9e96-317a1e3ed802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.96s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "annFile = '{}/annotations/captions_{}.json'.format(dataDir,dataType)\n",
        "coco_caps=COCO(annFile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkelDfFNLeeK"
      },
      "source": [
        "## Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n206e1hmLeeK",
        "outputId": "b2f17d99-b367-44b4-9e4d-f29ab2f59d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=8.66s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# initialize COCO API for instance annotations\n",
        "\n",
        "\"\"\"\n",
        "The code first imports the necessary libraries and sets the path to the MSCOCO dataset.\n",
        "It then initializes the COCO API for instance annotations by loading the annotation file.\n",
        "\n",
        "Next, it loads the image ids for all images in the dataset using the getImgIds() function.\n",
        "It then loops through each image id and loads the corresponding image data using the loadImgs(imgId) function.\n",
        "The image data is then appended to the list imgs.\n",
        "\n",
        "For each image, the code also loads the captions associated with the image using the getAnnIds() and\n",
        "loadAnns() functions. The captions are then appended to the list captions.\n",
        "\n",
        "The resulting imgs and captions lists can then be used for further processing, such as training a\n",
        "neural network for image captioning.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "annFile = '{}/annotations/instances_{}.json'.format(dataDir, dataType)\n",
        "coco = COCO(annFile)\n",
        "\n",
        "# load image ids\n",
        "imgIds = coco.getImgIds()\n",
        "\n",
        "# load images and captions\n",
        "imgs = []\n",
        "captions = []\n",
        "for imgId in imgIds:\n",
        "    # load image data\n",
        "    img = coco.loadImgs(imgId)[0]\n",
        "    imgs.append(img)\n",
        "    # load captions\n",
        "    annIds = coco.getAnnIds(imgIds=img['id'], catIds=coco.getCatIds())\n",
        "    anns = coco.loadAnns(annIds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWJRUWTBmkfm"
      },
      "source": [
        "## Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8HQDNGRVu-v"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code defines a Python class Data, which represents a single image in the dataset along with\n",
        "its associated captions.\n",
        "\n",
        "The class has an __init__ method, which is called when an object of the class is created.\n",
        "The method takes two arguments, id and captions, and assigns them to the class object's id and captions attributes. It also creates an attribute image, which is associated with the image data by calling the img_id_to_data(self.id) function.\n",
        "\n",
        "The class also has a method show_info(), which is used to display information about the\n",
        "image and its captions. This method calls the show_img_by_file(pictures_dir_path + \"/\" + img_id_to_url(self.id)\n",
        ") function to display the image, and then prints the captions associated with the image.\n",
        "\n",
        "This class can be used to create an object for each image in the dataset, with the image id and captions as\n",
        "arguments. The show_info() method can be called on each object to display the image and captions.\n",
        "\"\"\"\n",
        "class Data:\n",
        "    def __init__(self, id, captions):\n",
        "        self.id = id\n",
        "        self.captions = captions\n",
        "        self.image = img_id_to_data(self.id)\n",
        "    def show_info(self):\n",
        "        show_img_by_file(pictures_dir_path + \"/\" + img_id_to_url(self.id))\n",
        "        print(self.captions)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3B2x1VDmzDR"
      },
      "source": [
        "## Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0qvVmotm1n1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "This code defines several functions that are used to process and display images in the dataset.\n",
        "\n",
        "The show_img_by_file(file_address) function takes an argument file_address, which is the file path of an\n",
        "image. It uses the imshow() function from the matplotlib library to display the image. The axis('off')\n",
        "function is used to remove the axis from the displayed image and the show() function is used to show the image.\n",
        "\n",
        "\"\"\"\n",
        "def show_img_by_file(file_address):\n",
        "    plt.imshow(mpimg.imread(file_address))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\"\"\"\n",
        "The img_id_to_url(img_id, img_type='jpg') function takes an argument img_id, which is the unique identifier\n",
        "of an image, and an optional argument img_type, which is the image format. It converts the img_id to a\n",
        "string and pads it with zeroes so that it has 12 digits. It then concatenates the padded img_id with the\n",
        "image format and returns the resulting string.\n",
        "\n",
        "\"\"\"\n",
        "def img_id_to_url(img_id, img_type='jpg'):\n",
        "    str_id = str(img_id)\n",
        "    return str_id.zfill(12) + '.' + img_type\n",
        "\n",
        "\"\"\"\n",
        "The img_id_to_data(img_id, img_height=224, img_width=224) function takes an argument img_id, which\n",
        "is the unique identifier of an image, and optional arguments img_height and img_width, which are the\n",
        "height and width of the image. It calls the img_id_to_url(img_id) function to get the image file name,\n",
        "constructs the image file path by concatenating the file name with the path of the directory containing\n",
        "the images. It loads the image from the file path using the load_img() function from the keras library and\n",
        "resizes it to the specified dimensions. It converts the image to an array and normalizes the pixel values\n",
        "by dividing them by 255.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def img_id_to_data(img_id, img_height=224, img_width=224):\n",
        "    url = img_id_to_url(img_id)\n",
        "    img_path = pictures_dir_path+'/'+url\n",
        "    print()\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))\n",
        "  # img = mpimg.imread(pictures_dir_path+'/'+url)\n",
        "  # img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    input_arr = tf.keras.preprocessing.image.img_to_array(img)\n",
        "\n",
        "  # img /= 255.\n",
        "    input_arr /= 255.\n",
        "\n",
        "    return input_arr\n",
        "\"\"\"\n",
        "The show_image_by_data(img) function takes an argument img which is the image data and uses the imshow()\n",
        "function from the matplotlib library to display the image. The show() function is used to show the image.\n",
        "\n",
        "\"\"\"\n",
        "def show_image_by_data(img):\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\"\"\"\n",
        "The remove_startseq_endseq(captions) function takes a list of captions as an argument, removes the word\n",
        "'startseq' from the start of each caption and the word 'endseq' from the end of each caption. It returns\n",
        "the modified list of captions.\n",
        "\n",
        "\"\"\"\n",
        "def remove_startseq_endseq(captions):\n",
        "    captions = [caption.replace('startseq ', '') for caption in captions]\n",
        "    captions = [caption.replace(' endseq', '') for caption in captions]\n",
        "    return captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9A8A2F5S5Na"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_4qzHTLY8AT"
      },
      "source": [
        "## Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6MYXdwrZDOt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is defining a function called \"split_data\" that takes four arguments: \"source_dir\", \"train_dir\",\n",
        "\"test_dir\", and \"split_size\". The function first creates a list of items in the directory specified by\n",
        "\"source_dir\" and shuffles them randomly. Then, it iterates through the shuffled list of items, and copies\n",
        "the first \"split_size * len(items)\" items to the directory specified by \"train_dir\", and the remaining items\n",
        "to the directory specified by \"test_dir\". This function is used to split the dataset into training and test\n",
        "set.\n",
        "\"\"\"\n",
        "def split_data(source_dir, train_dir, test_dir, split_size):\n",
        "    items = os.listdir(source_dir)\n",
        "    items = random.sample(items, len(items))\n",
        "    for i in range(len(items)):\n",
        "        if i < split_size * len(items):\n",
        "            copyfile(os.path.join(source_dir, items[i]), os.path.join(train_dir, items[i]))\n",
        "        else:\n",
        "            copyfile(os.path.join(source_dir, items[i]), os.path.join(test_dir, items[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkkijsPldIAk",
        "outputId": "0f837f4e-8da4-4bb0-b7a6-e65e327e943e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82783 26190\n"
          ]
        }
      ],
      "source": [
        "# Run the line below just for the first time\n",
        "# split_data(pictures_dir_path, train_dir_path, test_dir_path, 0.8)\n",
        "print(len(os.listdir(train_dir_path)), len(os.listdir(test_dir_path)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "kfUYVoSsHBGH",
        "outputId": "d2582c67-7468-45e2-fa75-68ff1e912d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Lab_work/Video_Caption/training_caption/ImageCaptioning-main/dataset_MSCOCO/train2014'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1JLVlImfHzi"
      },
      "source": [
        "## Load Images and Captions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnXCmSWjfMf4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is loading image captions and image data from a source directory. The source directory is passed as\n",
        "an argument to the function load_imgs_and_caps(). It first loads the list of images in the source directory.\n",
        "Then, for each image in the directory, it converts the image's URL to an ID using the img_url_to_id() function,\n",
        "which is defined earlier in the code. It then uses the ID to load the captions of the image from a data\n",
        "source called coco_caps.\n",
        "It then calls the img_id_to_data(img_id) function to get the r,g,b values of the image. it creates several\n",
        "lists to store data of images such as ids, is_training, r,g,b, captions, and urls.\n",
        "\n",
        "Each image's data is then appended to the corresponding list, and a dataframe is returned that contains\n",
        "all of the image data.\n",
        "\n",
        "It also print the message 'X images loaded' every 200 images to indicate the progress of the function.\n",
        "\"\"\"\n",
        "def img_url_to_id(img_url, img_type='jpg'):\n",
        "    return str(img_url[:-len(img_type)-1])\n",
        "\n",
        "# def img_url_to_id(img_url, img_type='jpg'):\n",
        "    # return int(img_url.split('_')[-1].split('.')[0])\n",
        "\n",
        "def load_img_captions(img_id):\n",
        "    captions = []\n",
        "    annIds = coco_caps.getAnnIds(imgIds=img_id);\n",
        "    anns = coco_caps.loadAnns(annIds)\n",
        "    for ann in anns:\n",
        "        captions.append(ann['caption'])\n",
        "    return captions\n",
        "\n",
        "\n",
        "def load_imgs_and_caps(source_dir, is_training):\n",
        "  global global_data, global_train_data, get_test_data\n",
        "  images = os.listdir(source_dir)\n",
        "  counter = 0\n",
        "  # new\n",
        "  total_ids = []\n",
        "  total_is_trainings = []\n",
        "  total_rs = []\n",
        "  total_gs = []\n",
        "  total_bs = []\n",
        "  total_caps = []\n",
        "  total_urls = []\n",
        "  # new\n",
        "  for img_url in images:\n",
        "    # t1 = time.time()\n",
        "    img_id = img_url_to_id(img_url)\n",
        "    captions = load_img_captions(img_id)\n",
        "    # data = Data(id = img_id, captions = captions)\n",
        "    # new\n",
        "    total_ids.append(img_id)\n",
        "    total_is_trainings.append(is_training)\n",
        "\n",
        "    t1 = time.time()\n",
        "    img_data = img_id_to_data(img_id)\n",
        "\n",
        "    # t2 = time.time()\n",
        "    # print(f'1: {t2-t1}')\n",
        "\n",
        "    total_rs.append(img_data) ###\n",
        "    total_gs.append(img_data[1])\n",
        "    total_bs.append(img_data[2])\n",
        "    total_caps.append(captions)\n",
        "    total_urls.append(img_url)\n",
        "\n",
        "    t3 = time.time()\n",
        "    # print(f'2: {t3-t2}')\n",
        "    # new\n",
        "\n",
        "\n",
        "    # global_data.append(data)\n",
        "    if counter % 200 == 0:\n",
        "      print(f'{counter} images loaded')\n",
        "    counter += 1\n",
        "    # if is_training:\n",
        "    #   global_train_data.append(data)\n",
        "    #   train_pics.append(data.image)\n",
        "    #   # train_pics.append(data)\n",
        "    # else:\n",
        "    #   global_test_data.append(data)\n",
        "    #   test_pics.append(data.image)\n",
        "    #   # test_pics.append(data)\n",
        "    t3 = time.time()\n",
        "    # print(f'2: {t3-t2}')\n",
        "  return pd.DataFrame({\n",
        "        \"id\": total_ids,\n",
        "        \"is_training\": total_is_trainings,\n",
        "        'r': total_rs,\n",
        "        'g': total_gs,\n",
        "        'b': total_bs,\n",
        "        'captions': total_caps,\n",
        "        'url': total_urls\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk4ZJqlaWZvA"
      },
      "source": [
        "## Load and Save images to dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMu_Zy_quINH",
        "outputId": "d49b4bb1-ecd8-429b-cbb2-f55a31b2cba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "11800 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "12000 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "12200 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "12400 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "12600 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "12800 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "13000 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "13200 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "13400 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "13600 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "13800 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "14000 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "14200 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "14400 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "14600 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "14800 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "15000 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "15200 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "15400 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "15600 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "15800 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "16000 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "16200 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "16400 images loaded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "The code below load all images from the directories and save the required data\n",
        "to dataframes (train, test), and then save it for the further usage.\n",
        "\"\"\"\n",
        "train_df = load_imgs_and_caps(train_dir_path, True)\n",
        "train_df.to_csv(os.path.join(project_path,\"train_data_coco.csv\"), sep=',')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cegm2p-YaohR"
      },
      "outputs": [],
      "source": [
        "test_df = load_imgs_and_caps(test_dir_path, False)\n",
        "test_df.to_csv(os.path.join(project_path,\"test_data_coco.csv\"), sep=',')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAS54pAXW_wy"
      },
      "source": [
        "## Read data from dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlpLmJaNXD_J"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_df_path)\n",
        "test_df = pd.read_csv(test_df_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx8ApoqDZBzD"
      },
      "source": [
        "### Delete extra rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A21KRPR7X9td"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop(columns=['Unnamed: 0'])\n",
        "test_df = test_df.drop(columns=['Unnamed: 0'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxwpqOsQbVEP"
      },
      "source": [
        "### Get Image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPOLIELgbXoJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is taking a string as input, which represents an array of numbers. It is then editing the\n",
        "string by removing certain characters (e.g. \"[\", \"\\n\", \"]\") and replacing multiple spaces with a\n",
        "single space. Then it converts the edited string into an array of numbers using the numpy function\n",
        "np.fromstring() . Finally it reshape the array of numbers to a desired shape (default is [224,224])\n",
        "and return\n",
        "the reshaped array.\n",
        "\"\"\"\n",
        "def string_to_array(raw_str, shape = [224,224]):\n",
        "\n",
        "  edited = raw_str.replace('[', '')\n",
        "  edited = edited.replace('\\n','')\n",
        "  edited = edited.replace(']', '')\n",
        "  edited = re.sub(' +', ' ', edited)\n",
        "  edited = np.fromstring(edited, dtype=float, sep=' ')\n",
        "  return edited.reshape(shape)\n",
        "  # print(edited)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5cfVXu-hDWq"
      },
      "outputs": [],
      "source": [
        "# string_to_array(train_df['r'][0])\n",
        "\"\"\"\n",
        "his code defines a function all_data() which takes a list of image ids as input. It then creates\n",
        "an empty list data and a counter variable counter. It then iterates over the ids in the input list,\n",
        "and for each id, it calls the function img_id_to_data() with the id as input, and appends the returned\n",
        "data to the data list. Every 200 iterations, it prints the message of how many images have been loaded.\n",
        "Finally,\n",
        "it returns the data list.\n",
        "\"\"\"\n",
        "def all_data(ids):\n",
        "  data = []\n",
        "  counter = 0\n",
        "  for id in ids:\n",
        "    data.append(img_id_to_data(id))\n",
        "    if counter % 200 == 0:\n",
        "      print(f'{counter} images loaded')\n",
        "    counter += 1\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7TvPzV0kJ-7"
      },
      "source": [
        "## New way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb-VPiMDkObQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is a function that preprocesses an image. It loads an image from a file path,\n",
        "resizes it to 299x299 pixels, converts it to a numpy array, adds an extra dimension to the array,\n",
        "and applies preprocessing to the image using the preprocess_input() function from the inception_v3\n",
        "module in TensorFlow's Keras library. The preprocessing is specific to the Inception V3 model so that\n",
        "the image is ready to be passed through the model.\n",
        "\n",
        "\"\"\"\n",
        "def preprocess(image_path):\n",
        "    # Convert all the images to size 299x299 as expected by the inception v3 model\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
        "    # Convert PIL image to numpy array of 3-dimensions\n",
        "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    # Add one more dimension\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    # preprocess the images using preprocess_input() from inception module\n",
        "    x = tf.keras.applications.inception_v3.preprocess_input(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzOAx3VkkWDE"
      },
      "outputs": [],
      "source": [
        "# Load the inception v3 model\n",
        "input1 = tf.keras.applications.InceptionV3(weights='imagenet')\n",
        "\n",
        "# Create a new model, by removing the last layer (output layer) from the inception v3\n",
        "pt_model = tf.keras.models.Model(input1.input, input1.layers[-2].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ou_p8O9lYiI"
      },
      "outputs": [],
      "source": [
        "# Function to encode a given image into a vector of size (2048, )\n",
        "\"\"\"\n",
        "This code is defining a function called \"encode\" that takes in an image as an input.\n",
        "It first runs the image through a preprocessing step, which includes loading the image,\n",
        "converting it to a numpy array, adding an extra dimension, and using a preprocessing function from a\n",
        "module called \"inception_v3\" to adjust the image.\n",
        "\n",
        "After the image is preprocessed, the function gets the \"encoding vector\" for the image by running the\n",
        "preprocessed image through a model called \"pt_model\" and predicting the encoding vector.\n",
        "\n",
        "The encoding vector is then reshaped so that its shape is (2048, ) instead of (1, 2048) before it is\n",
        "returned by the function.\n",
        "\"\"\"\n",
        "def encode(image):\n",
        "    image = preprocess(image) # preprocess the image\n",
        "    fea_vec = pt_model.predict(image) # Get the encoding vector for the image\n",
        "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
        "    return fea_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5csYrudZJ5l8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Run the code below for the first run\n",
        "\"\"\"\n",
        "\n",
        "t_encoding = {}\n",
        "for url in tqdm.tqdm(test_df['url']):\n",
        "    t_encoding[url] = encode(pictures_dir_path+'/'+url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcmrMMeqKK1R"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Run the code below for the first run\n",
        "\"\"\"\n",
        "# Save the features in the images1 pickle file\n",
        "with open(os.path.join(project_path,\"test_images_coco.pkl\"), \"wb\") as encoded_pickle:\n",
        "    pickle.dump(encoding, encoded_pickle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi-Idm2GNDN2"
      },
      "outputs": [],
      "source": [
        "t_features = pickle.load(open(os.path.join(project_path, \"test_images_coco.pkl\"), \"rb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDh8Phf9OHUY"
      },
      "source": [
        "### Train features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU4dLOOjlg-X"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Run the code below for the first run\n",
        "\"\"\"\n",
        "encoding = {}\n",
        "for url in tqdm.tqdm(train_df['url']):\n",
        "    encoding[url] = encode(pictures_dir_path+'/'+url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvcodDgsuBaC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Run the code below for the first run\n",
        "\"\"\"\n",
        "# Save the features in the images1 pickle file\n",
        "with open(os.path.join(project_path,\"train_images_coco.pkl\"), \"wb\") as encoded_pickle:\n",
        "    pickle.dump(encoding, encoded_pickle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzAES4NjuWFW"
      },
      "outputs": [],
      "source": [
        "# print(encoding['000000000139.jpg'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxlyMrR-ziYC"
      },
      "source": [
        "### load features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF8f4X62zTc-"
      },
      "outputs": [],
      "source": [
        "features = pickle.load(open(os.path.join(project_path,\"train_images_coco.pkl\"), \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLEHvaywzpoD"
      },
      "outputs": [],
      "source": [
        "# testing\n",
        "print(features['000000308587.jpg'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07cnsSTYz7iZ"
      },
      "source": [
        "# Captions Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqFIdJVG0CaL"
      },
      "source": [
        "## Build Vocabularies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxNmeeYOqqUX"
      },
      "source": [
        "### Load captions (endseq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW-pGN2dnVuW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code defines a function called add_start_and_endseq that takes in a string as its input.\n",
        "The function concatenates the string \"startseq\" to the beginning of the input string, and\n",
        "concatenates \"endseq\" to the end of the input string, with a space in between. The function\n",
        "then returns the\n",
        "resulting concatenated string.\n",
        "\"\"\"\n",
        "def add_start_and_endseq(input_str):\n",
        "  return 'startseq' + ' ' + input_str + ' ' + 'endseq'\n",
        "\n",
        "#Load caption and add endseq at the end of them\n",
        "\"\"\"\n",
        "This code creates a dictionary of captions for a set of images. The captions are obtained from the\n",
        "input_coco_caps object, which is assumed to be a COCO object with captions. The images are located\n",
        "in the directory specified by the dir_path variable (default is train_dir_path). For each image,\n",
        "the annotations associated with the image are retrieved, and for each annotation, the caption is\n",
        "modified by adding 'startseq' at the beginning and 'endseq' at the end, and then added to the\n",
        "captions_dic dictionary, with the image_id as the key. If the image_id already exists in the\n",
        "dictionary, the new caption is added to the list of captions associated with that image_id.\n",
        "\"\"\"\n",
        "def create_captions_dic(inp_coco_caps, dir_path = train_dir_path):\n",
        "  captions_dic = {}\n",
        "  images = os.listdir(dir_path)\n",
        "  for img_url in images:\n",
        "    img_id = img_url_to_id(img_url)\n",
        "    annIds2 = inp_coco_caps.getAnnIds(imgIds=img_id)\n",
        "    anns2 = inp_coco_caps.loadAnns(annIds2)\n",
        "    counter = 0\n",
        "    for ann in anns2:\n",
        "      counter += 1\n",
        "      # there are 13 images with 6 captions, not 5\n",
        "      # if counter > 5:\n",
        "      #   print (img_id)\n",
        "      edited_caption = add_start_and_endseq(ann['caption'])\n",
        "      if ann['image_id'] not in captions_dic:\n",
        "        captions_dic[ann['image_id']] = [edited_caption]\n",
        "      else:\n",
        "        captions_dic[ann['image_id']].append(edited_caption)\n",
        "\n",
        "  return captions_dic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MliGFsoirHGe"
      },
      "outputs": [],
      "source": [
        "train_caps = create_captions_dic(coco_caps)\n",
        "\n",
        "#Test\n",
        "print(train_caps[289516])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaHz30HwHo5c"
      },
      "outputs": [],
      "source": [
        "test_caps = create_captions_dic(coco_caps, dir_path=test_dir_path)\n",
        "\n",
        "#Test\n",
        "print(len(test_caps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhpZQPmZ02tH"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDcaF06qzKC-"
      },
      "source": [
        "#### Making a list of all captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpSSvEEy1QZG"
      },
      "outputs": [],
      "source": [
        "len(train_caps.values())\n",
        "flat_train_caps = [cap for sublist in train_caps.values() for cap in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L5ISveUxxLb"
      },
      "outputs": [],
      "source": [
        "print(len(flat_train_caps))\n",
        "print(len(train_caps.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2rq98I1zRLM"
      },
      "source": [
        "#### Tokenizing captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLIwWgve06Ri"
      },
      "outputs": [],
      "source": [
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(oov_token='OOV',\n",
        "                      filters= \"!'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\",\n",
        "                      num_words=num_words)\n",
        "\n",
        "# Generate the word index dictionary\n",
        "tokenizer.fit_on_texts(flat_train_caps)\n",
        "\n",
        "# Define the total words. You add 1 for the index `0` which is just the padding token.\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f'word index dictionary: {tokenizer.word_index}')\n",
        "print(f'total words: {total_words}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taQ3gVmCA9UR"
      },
      "outputs": [],
      "source": [
        "# Testing the tokenizer\n",
        "tseq = tokenizer.texts_to_sequences(['Let us play some ball', 'asdlk ha sdjh askdjhasd hkajsd'])\n",
        "print(tseq)\n",
        "print(tseq[0][:2], tseq[0][2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFbK_qEpe-a9"
      },
      "outputs": [],
      "source": [
        "cap_lengths = [len(item.split(' ')) for item in flat_train_caps]\n",
        "result = np.where(cap_lengths == np.amax(cap_lengths))\n",
        "cap_lengths.sort(reverse = True)\n",
        "len(cap_lengths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7jzg-zpbqxg"
      },
      "source": [
        "### Download Glove pretrained embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJpjfOGibu_u"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "only first time\n",
        "\"\"\"\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-mLqdVVdCyL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Only the first time\n",
        "\"\"\"\n",
        "# !unzip /home/hp/PycharmProjects/Usama/ImageCaptioning-main/glove.6B.zip -d /home/hp/PycharmProjects/Usama/ImageCaptioning-main/Glove/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD9IZ0s08bfa"
      },
      "outputs": [],
      "source": [
        "# tword, *tvector = 'Hey man SUP? wyd'.split()\n",
        "# print(tword,tvector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX_DgyHxgARr"
      },
      "outputs": [],
      "source": [
        "# def embedding_for_vocab(filepath, word_index,\n",
        "#                         embedding_dim):\n",
        "#   # num _words instead of word index length\n",
        "#   vocab_size = num_words + 1\n",
        "\n",
        "#   # Adding again 1 because of reserved 0 index\n",
        "#   embedding_matrix_vocab = np.zeros((vocab_size,\n",
        "#                                       embedding_dim))\n",
        "\n",
        "#   with open(filepath, encoding=\"utf8\") as f:\n",
        "#     for line in f:\n",
        "#       word, *vector = line.split()\n",
        "#       # adding index if\n",
        "#       if word in word_index and word_index[word] < num_words:\n",
        "#         idx = word_index[word]\n",
        "#         embedding_matrix_vocab[idx] = np.array(\n",
        "#             vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "#     return embedding_matrix_vocab\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NepUHwZY9qS-"
      },
      "outputs": [],
      "source": [
        "# embedding_matrix = embedding_for_vocab('/home/hp/PycharmProjects/Usama/ImageCaptioning-main/Glove/glove.6B.100d.txt', tokenizer.word_index, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahf4OUkq-k_N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkzqxjZChEwe"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fgni5wuXhHc4"
      },
      "outputs": [],
      "source": [
        "def n_gram_seqs(corpus, tokenizer):\n",
        "    \"\"\"\n",
        "    Generates a list of n-gram sequences\n",
        "\n",
        "    Args:\n",
        "        corpus (list of string): lines of texts to generate n-grams for\n",
        "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
        "\n",
        "    Returns:\n",
        "        input_sequences (list of int): the n-gram sequences for each line in the corpus\n",
        "    \"\"\"\n",
        "    input_sequences = []\n",
        "\n",
        "    ### START CODE HERE\n",
        "    for line in corpus:\n",
        "      current_sequence = tokenizer.texts_to_sequences([line])[0]\n",
        "      for i in range(1, len(current_sequence)):\n",
        "        input_sequences.append(current_sequence[0: i+1])\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKpLbUTKvzFg"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code defines a function \"train_generator\" that is used to generate batches of data for training a model.\n",
        "The function takes in three inputs:\n",
        "\n",
        "tokenizer: which is used to convert text to numerical sequences\n",
        "captions: a dictionary that contains image captions, where the key is the image id and the value is a\n",
        "list of captions for that image\n",
        "photos: a dictionary that contains image features, where the key is the image url and the value is the\n",
        "feature vector for that image\n",
        "batch_size: the number of images to include in each batch of data\n",
        "The function is an infinite loop that goes through each image id in the captions dictionary and\n",
        "retrieves the associated captions and photo feature. It then converts the captions to numerical\n",
        "sequences using the tokenizer, and pads the input sequences to a fixed length. Then, it splits the\n",
        "sequences into input and output pairs and encodes the output sequences. It appends the image feature,\n",
        "input sequence and output sequence to the corresponding lists. Once the number of images processed is\n",
        "equal to the batch size, it yields the lists as numpy arrays in the format of\n",
        "([image_features, input_sequences], output_sequences).\n",
        "\"\"\"\n",
        "def train_generator(tokenizer, captions, photos, batch_size):\n",
        "  image_inp, sequence_inp, out_word = list(), list(), list()\n",
        "  n=0\n",
        "  # loop for ever over images\n",
        "  while True:\n",
        "      for img_id, caps_list in train_caps.items():\n",
        "          n+=1\n",
        "          # retrieve the photo feature\n",
        "          photo = photos[img_id_to_url(img_id)]\n",
        "          for cap in caps_list:\n",
        "              # encode the sequence\n",
        "              seq = tokenizer.texts_to_sequences([cap])[0]\n",
        "              # print(seq)\n",
        "              # return\n",
        "              # seq = [tokenizer.word_index[word] for word in cap.split(' ') if word in tokenizer.word_index]\n",
        "\n",
        "              # split one sequence into multiple X, y pairs\n",
        "              for i in range(1, min(max_length+1, len(seq))):\n",
        "                  # split into input and output pair\n",
        "                  in_seq, out_seq = seq[:i], seq[i]\n",
        "                  # pad input sequence\n",
        "                  in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                  # encode output sequence\n",
        "                  out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=num_words)[0]\n",
        "                  # store\n",
        "                  image_inp.append(photo)\n",
        "                  sequence_inp.append(in_seq)\n",
        "                  out_word.append(out_seq)\n",
        "\n",
        "                  # print(in_seq, out_seq)\n",
        "\n",
        "          # yield the batch data\n",
        "          if n==batch_size:\n",
        "            # print(image_inp[-1])\n",
        "            yield ([np.array(image_inp), np.array(sequence_inp)], np.array(out_word))\n",
        "            image_inp, sequence_inp, out_word = list(), list(), list()\n",
        "            n=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q6YNsRYo7so"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is defining a neural network model with two inputs, one input is an image represented by\n",
        "a 2048-dimensional vector, the other input is a sequence of words represented by a max_length-dimensional\n",
        "vector. The model has several layers:\n",
        "\n",
        "A Dropout layer with a rate of 0.5 is applied to the image input to prevent overfitting.\n",
        "A dense layer with 512 neurons and a ReLU activation function is applied to the output of the dropout layer.\n",
        "An Embedding layer is applied to the sequence input, it converts the words in the sequence to a dense\n",
        "embedding of size embedding_dim and set mask_zero = True which means any input with value zero will be masked.\n",
        "A Dropout layer with a rate of 0.5 is applied to the output of the embedding layer.\n",
        "A LSTM layer with 512 neurons is applied to the output of the second dropout layer.\n",
        "The outputs of the dense layer and the LSTM layer are combined using the add function.\n",
        "A dense layer with 256 neurons and a ReLU activation function is applied to the output of the add function.\n",
        "Finally, a dense layer with num_words neurons and a softmax activation function is applied to the\n",
        "output of the previous dense layer, this layer produces the final output of the model which is a probability distribution over all possible words in the vocabulary.\n",
        "\"\"\"\n",
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(512, activation='relu')(fe1)\n",
        "\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(num_words, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(512)(se2)\n",
        "\n",
        "decoder1 = add([fe2, se3])\n",
        "# decoder1 = add([inputs1, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(num_words, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9qgvP0pSIuE"
      },
      "outputs": [],
      "source": [
        "tokenizer.word_counts['chrome']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg7nJgEMuy4s"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 8\n",
        "steps = len(train_caps)//batch_size\n",
        "\n",
        "# https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "for i in range(epochs):\n",
        "  print(f'Epoch {i+1}/{epochs}')\n",
        "  generator = train_generator(tokenizer, train_caps, features, batch_size)\n",
        "  model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "model_name = 'final_model_coco_V5' + '.h5'\n",
        "model_file_path = os.path.join(project_path, model_name)\n",
        "model.save(model_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq_VNnWnSA5N"
      },
      "outputs": [],
      "source": [
        "# model.save('./model_final' + '.h5')\n",
        "\n",
        "# final_model = model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9rVLovcSPal"
      },
      "source": [
        "# Caption Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J720g1nXnK-0"
      },
      "source": [
        "## Loading the MLE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6I_EJgjmvPt"
      },
      "outputs": [],
      "source": [
        "model_file = os.path.join(project_path, 'final_model_coco_V5.h5')\n",
        "print(model_file)\n",
        "final_model = tf.keras.models.load_model(model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N12CCs1AnPmn"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.python.ops.gen_array_ops import stop_gradient\n",
        "\"\"\"\n",
        "This code defines a function generate_caption that takes in the following parameters:\n",
        "\n",
        "tokenizer: An object that can be used to tokenize text\n",
        "picture: An input image\n",
        "max_length: The maximum length of the generated caption\n",
        "model: The model that will be used to generate the caption\n",
        "The function starts by initializing a variable generated_text with the value 'startseq'.\n",
        "It then enters a for-loop that runs for a maximum of max_length times.\n",
        "\n",
        "In each iteration of the loop, the function tokenizes the current value of generated_text, pads it\n",
        "to the maximum length and uses the model to predict the next word of the caption. The function then\n",
        "selects the word with the highest probability and appends it to generated_text.\n",
        "\n",
        "When the new word is 'endseq' or the loop has reached its maximum iteration, the function replaces\n",
        "'startseq' and 'endseq' in the generated text and returns the final caption.\n",
        "\"\"\"\n",
        "def generate_caption(tokenizer, picture, max_length, model):\n",
        "    generated_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "      split_generated_text = [generated_text]\n",
        "      # print('0.', split_generated_text)\n",
        "\n",
        "      sequence = tokenizer.texts_to_sequences(split_generated_text)\n",
        "\n",
        "      # print('1.', sequence, sequence[0])\n",
        "      # return\n",
        "\n",
        "      sequence = pad_sequences(sequence, maxlen=max_length)\n",
        "      # sequence = np.array(sequence).flatten()\n",
        "      # print('2.sequence: ', sequence)\n",
        "\n",
        "      word_probs = model.predict([picture,sequence], verbose=0)\n",
        "\n",
        "      # print('3.')\n",
        "\n",
        "      # print(np.max(word_probs), np.min(word_probs))\n",
        "\n",
        "      # dist = tfp.distributions.Categorical(probs=word_probs, dtype=tf.float32)\n",
        "      # tf_predicted_index = dist.sample()\n",
        "      # predicted_index = np.array(tf_predicted_index)\n",
        "      predicted_index = [np.argmax(word_probs)]\n",
        "\n",
        "\n",
        "      # print('4.', predicted_index)\n",
        "\n",
        "\n",
        "      new_word = tokenizer.sequences_to_texts([predicted_index])[0]\n",
        "      generated_text += ' ' + new_word\n",
        "      # print('end', new_word)\n",
        "      # return\n",
        "      if new_word == 'endseq':\n",
        "          break\n",
        "    caption = generated_text.replace('startseq', '')\n",
        "    caption = caption.replace(' endseq', '')\n",
        "    # final = final[1:-1]\n",
        "    # final = ' '.join(final)\n",
        "    return caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBFga9ptPgOE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "his code defines a function called \"compute_bleu\" that takes three inputs: an image url, a generated\n",
        "caption, and an optional input called \"for_testing\" which is set to \"True\" by default.\n",
        "\n",
        "The function first converts the image url to an image id using a helper function \"img_url_to_id\".\n",
        "\n",
        "Then, it checks the value of the \"for_testing\" input. If it is \"True\", the function uses a pre-defined\n",
        "dictionary called \"test_caps\" to retrieve the captions for the image with the id obtained earlier.\n",
        "If \"for_testing\" is not \"True\", it uses another pre-defined dictionary called \"train_caps\" to retrieve\n",
        "the captions.\n",
        "\n",
        "The function then does some pre-processing on the captions, removing the text \"startseq\" and \"endseq\"\n",
        "from the start and end of each caption.\n",
        "\n",
        "Then the function calculates the BLEU score between the generated caption and the original captions\n",
        "using the sentence_bleu function from the nltk library. The smoothing_function parameter is set to\n",
        "\"SmoothingFunction(epsilon=1e-12).method1\" which is a smoothing method that uses epsilon = 1e-12.\n",
        "The function returns the BLEU score.\n",
        "\"\"\"\n",
        "def compute_bleu(img_url, generated_caption, for_testing = True):\n",
        "\n",
        "  img_id = img_url_to_id(img_url)\n",
        "  if for_testing:\n",
        "    captions = test_caps[img_id]\n",
        "  else:\n",
        "    captions = train_caps[img_id]\n",
        "  captions = [caption.replace('startseq ', '') for caption in captions]\n",
        "  captions = [caption.replace(' endseq', '') for caption in captions]\n",
        "  # print(captions)\n",
        "  generated_caption = generated_caption.replace('startseq ', '')\n",
        "  generated_caption = generated_caption.replace(' endseq', '')\n",
        "  # print(captions)\n",
        "\n",
        "  return sentence_bleu(captions, generated_caption, smoothing_function= SmoothingFunction(epsilon=1e-12).method1)\n",
        "\n",
        "# compute_bleu('000000336209.jpg', 'a shirtless man is skating on a steep')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCyNXtgZpYQ9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is a function called \"show_result\" that takes in an index of an\n",
        "image and an optional parameter for the file path of where the images are located.\n",
        "The function uses the index to find the corresponding image file name from a list\n",
        "of keys of a dictionary containing image features. It then takes that image and\n",
        "reshapes it to the correct dimensions and uses it as an input to generate a\n",
        "caption using another provided function called \"generate_caption\". This generated\n",
        "caption is then used as an input along with the image file name to another provided\n",
        "function called \"compute_bleu\" which calculates the BLEU score for the generated caption.\n",
        "Finally, the image is displayed using plt.imshow() and the generated caption and BLEU\n",
        "score are printed out.\n",
        "\"\"\"\n",
        "def show_result(img_index, image_path = test_dir_path):\n",
        "  pic = list(t_features.keys())[img_index]\n",
        "  # print(pic[:-4])\n",
        "  # return\n",
        "  image = t_features[pic].reshape((1,2048))\n",
        "  x = plt.imread(test_dir_path+ '/' + pic)\n",
        "  plt.imshow(x)\n",
        "  plt.show()\n",
        "  caption = generate_caption(tokenizer, image, max_length, final_model)\n",
        "  bleu_score = compute_bleu(pic, caption)\n",
        "\n",
        "  print(\"Caption:\", caption)\n",
        "  print(\"BLEU score:\", bleu_score)\n",
        "\n",
        "show_result(0)\n",
        "show_result(10)\n",
        "show_result(random.randint(0,1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ2nkyglKX8Y"
      },
      "source": [
        "\n",
        "# Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z52mClSRKfm2"
      },
      "source": [
        "## Creating Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV4ft43mKW6U"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This is a class called ICEnv, which is used to create a custom environment for an image captioning task.\n",
        "The class inherits from the Env class of OpenAI Gym, which is a library that provides a standardized\n",
        "interface for creating and interacting with environments for reinforcement learning.\n",
        "\n",
        "\n",
        "The __init__ method is used to initialize the environment by setting the initial state, action space,\n",
        "and observation space. It takes several arguments, such as the initial image, an ID for the image,\n",
        "the maximum length of the caption, the number of words in the vocabulary, the start and end sequences\n",
        "for the caption, and a tokenizer to convert between word sequences and text.\n",
        "\n",
        "The step method is used to take an action in the environment and return the new state, reward, and\n",
        "whether the environment is done. The action is an integer representing the next word in the caption,\n",
        "and the method updates the state by inserting the word into the caption and calculating the reward\n",
        "based on the quality of the caption using BLEU score.\n",
        "\n",
        "The reset method is used to reset the environment to its initial state, which includes the initial image\n",
        "and caption. It takes optional arguments to change the image and ID.\n",
        "\n",
        "The render method is used to display the current state of the environment, which includes the\n",
        "current caption and optionally the image.\n",
        "\n",
        "This code is basically creating a custom environment for image captioning task, where agent can\n",
        "generate captions by taking actions, the environment will provide a reward based on the quality\n",
        "of the caption generated using BLEU score which agent will try to maximize.\n",
        "\"\"\"\n",
        "class ICEnv(Env):\n",
        "  def __init__(self, image, img_id, max_len, num_words, startseq, endseq,\n",
        "               tokenizer):\n",
        "    self.prev_rewards = []\n",
        "    self.reward = 0\n",
        "    self.total_rewards = 0\n",
        "    self.max_len = max_len\n",
        "    self.num_words = num_words\n",
        "    self.startseq = startseq\n",
        "    self.endseq = endseq\n",
        "    self.done = False\n",
        "    self.img_id = img_id\n",
        "    self.tokenizer = tokenizer\n",
        "    self.seq_len = 1\n",
        "\n",
        "    self.observation_space = {'image': spaces.Box(low = 0, high = 255,\n",
        "                                                  shape=(2048,)),\n",
        "                              \"words\": spaces.Box(low = 0, high = 2075,\n",
        "                                                  shape=(self.max_len,))}\n",
        "\n",
        "    self.action_space = spaces.Discrete(self.num_words, start = 1)\n",
        "\n",
        "    self.state = {'image': image,\n",
        "                  \"words\": np.pad([self.startseq], (0, self.max_len -1),\n",
        "                                  'constant' , constant_values=(0,0))}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    info = {}\n",
        "\n",
        "    \"\"\" Might need to change \"\"\"\n",
        "    # Update previous reward\n",
        "    self.prev_rewards.append(self.reward)\n",
        "\n",
        "    # Apply action\n",
        "    for i, word in enumerate(self.state['words']):\n",
        "      if word == 0:\n",
        "        self.state['words'][i] = action\n",
        "        self.seq_len += 1\n",
        "        break\n",
        "\n",
        "    # Calculate reward\n",
        "    ind_caption = []\n",
        "    for word in self.state['words']:\n",
        "        if word == self.endseq or word == 0:\n",
        "          break\n",
        "        elif word == self.startseq:\n",
        "          continue\n",
        "        else:\n",
        "          ind_caption.append(word)\n",
        "\n",
        "    caption = tokenizer.sequences_to_texts([ind_caption])[0]\n",
        "    references = train_caps[self.img_id]\n",
        "    self.reward = compute_bleu(img_id_to_url(self.img_id), caption, False)\n",
        "    # sentence_bleu(references, caption)\n",
        "    self.total_rewards += self.reward\n",
        "\n",
        "\n",
        "    # Check if it's done\n",
        "    if action == self.endseq or self.seq_len == self.max_len:\n",
        "      self.done = True\n",
        "\n",
        "\n",
        "    return self.state, self.reward, self.done, info\n",
        "\n",
        "\n",
        "  def reset(self, image = None, img_id = None):\n",
        "    self.prev_rewards = []\n",
        "    self.reward = 0\n",
        "    self.total_rewards = 0\n",
        "    self.done = False\n",
        "    self.seq_len = 1\n",
        "\n",
        "    self.state[\"words\"]= np.pad([self.startseq], (0, self.max_len -1),\n",
        "                                  'constant' , constant_values=(0,0))\n",
        "\n",
        "\n",
        "    if image != None and img_id != None:\n",
        "      self.state['image'] = image\n",
        "      self.img_id = img_id\n",
        "\n",
        "    return self.state\n",
        "\n",
        "  def render(self, with_image = False):\n",
        "    if with_image:\n",
        "      img_path = pictures_dir_path + '/' + img_id_to_url(self.img_id)\n",
        "      plt.imshow(plt.imread(img_path))\n",
        "      plt.show()\n",
        "\n",
        "    ind_caption = []\n",
        "    for word in self.state['words']:\n",
        "      if word == 0:\n",
        "        break\n",
        "      else:\n",
        "        ind_caption.append(word)\n",
        "    caption = tokenizer.sequences_to_texts([ind_caption])[0]\n",
        "    print(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEr16XwllOcM"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This object is then used to interact with an environment where a language model generates captions\n",
        "for images. The ICEnv class's step() method allows the language model to select a word at a time,\n",
        "and the reset() method allows to switch the image the caption is\n",
        "generated for.\n",
        "\"\"\"\n",
        "tmp_img_id = list(train_caps.keys())[0]\n",
        "tmp_img = features[img_id_to_url(tmp_img_id)]\n",
        "\n",
        "env = ICEnv(image= tmp_img, img_id= tmp_img_id,\n",
        "            max_len=max_length, num_words=num_words,\n",
        "            startseq = tokenizer.word_index['startseq'],\n",
        "            endseq = tokenizer.word_index['endseq'], tokenizer= tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em87hEhrmjKP"
      },
      "outputs": [],
      "source": [
        "env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWyZ7KkTohQm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is running a loop for a specified number of episodes (episodes) and in each episode, it:\n",
        "\n",
        "Resets the environment by calling the reset() function on the env object, which sets the initial\n",
        "state of the environment.\n",
        "Sets a flag done to False\n",
        "Initializes a score variable to 0\n",
        "Renders the environment with an image by calling render(with_image=True) on the env object\n",
        "It enters into a while loop which runs until the done flag is True.\n",
        "In each iteration of the while loop:\n",
        "it renders the environment by calling render() on the env object.\n",
        "it prints the current state of the environment, which is the list of words in the current sentence\n",
        "and the length of the list\n",
        "it samples an action from the action space of the environment and assigns the value to the variable action\n",
        "it takes the action by calling the step(action) function on the env object, which returns the new state,\n",
        "reward, done flag and additional information\n",
        "it adds the reward to the score\n",
        "After the while loop, it prints the episode number and the final score.\n",
        "\"\"\"\n",
        "episodes = 1\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    env.render(with_image = True)\n",
        "    while not done:\n",
        "        env.render()\n",
        "        print('state is:',env.state['words'], 'state len is:', len(env.state['words']))\n",
        "        action = env.action_space.sample()\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score+=reward\n",
        "    print('Episode:{} Score:{}'.format(episode, score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kran_SQqyl-_"
      },
      "source": [
        "# Final Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yt7ohDXESAq"
      },
      "source": [
        "Generator model is already pretrained using MLE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv9sP0VSEYIt"
      },
      "source": [
        "## Generating fake samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSRZfPVfyuAl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "For the first time\n",
        "\"\"\"\n",
        "### Generating fake samples\n",
        "# print('here')\n",
        "fake_captions = {}\n",
        "for img_url in tqdm.tqdm(features.keys()):\n",
        "  image = features[img_url].reshape((1,2048))\n",
        "\n",
        "  fake_cap = generate_caption(tokenizer, image,\n",
        "                              max_length, final_model)\n",
        "\n",
        "  fake_captions[img_url] = fake_cap\n",
        "### Save the features in the images1 pickle file\n",
        "\n",
        "with open(os.path.join(project_path,\"fake_captions_coco.pkl\"), \"wb\") as fake_caps_pickle:\n",
        "    pickle.dump(fake_captions, fake_caps_pickle)\n",
        "\n",
        "\"\"\"\n",
        "Fo the other times\n",
        "\"\"\"\n",
        "  # Loading saved captions\n",
        "# fake_captions = pickle.load(open(\"fake_captions.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7fQ3zPdFBmm"
      },
      "source": [
        "## Pre-training Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF9CjCfg7Kh6"
      },
      "source": [
        "### Creating Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXW_ZEAv4wGM"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code defines a function called \"create_discriminator()\" that creates a neural network model.\n",
        "The function takes no input, but it creates two inputs inside the function, one is named \"img_features\"\n",
        "which is an input layer and the\n",
        "shape of this input is 1, 2048. The other input is named \"caption\" and it has shape (max_length,).\n",
        "\n",
        "The function then applies an Embedding layer on the caption input and the output is embedded_caption,\n",
        "this layer is used to reduce the dimensionality of the input and learn a dense representation of the input.\n",
        "\n",
        "Then, it concatenates the img_features and embedded_caption along the axis 1, and the output of this\n",
        "concatenation is fed into an LSTM layer with 512 units.\n",
        "\n",
        "Then, the output of the LSTM layer is passed through a dense layer with 512 units and an activation\n",
        "function of \"leaky_relu\". The output of this dense layer is passed through a dropout layer with a rate\n",
        "of 0.4 to reduce overfitting, and the final output of the model is passed through a dense layer with 1\n",
        "unit and sigmoid activation function.\n",
        "\n",
        "The inputs of the model are img_features and caption, and the output is the final output of the model,\n",
        "and the model is returned by the function.\n",
        "\"\"\"\n",
        "def create_discriminator():\n",
        "  img_features = Input(shape=(1,2048,))\n",
        "\n",
        "  caption = Input(shape=(max_length,))\n",
        "  embedded_caption = Embedding(num_words, embedding_dim, mask_zero=True)(caption)\n",
        "\n",
        "\n",
        "  # disc_input = add([img_features, embedded_caption])\n",
        "  disc_input = tf.concat([img_features, embedded_caption], axis=1)\n",
        "\n",
        "  disc_lstm = LSTM(512)(disc_input)\n",
        "  disc_dense = Dense(512, activation='leaky_relu')(disc_lstm)\n",
        "  disc_dropout = Dropout(0.4)\n",
        "  outputs = Dense(1, activation='sigmoid')(disc_dense)\n",
        "\n",
        "  disc_model = Model(inputs=[img_features, caption], outputs=outputs)\n",
        "  # model.summary()\n",
        "\n",
        "  return disc_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l9bUc_S6anu"
      },
      "outputs": [],
      "source": [
        "disc = create_discriminator()\n",
        "# print(disc.layers[2].weights, disc.layers[2].trainable)\n",
        "disc.layers[2].set_weights(final_model.layers[2].weights)\n",
        "disc.layers[2].trainable = False\n",
        "disc.summary()\n",
        "# print(disc.layers[2].weights, disc.layers[2].trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9gsFRkE6UHY"
      },
      "outputs": [],
      "source": [
        "def load_discriminator():\n",
        "  disc = tf.keras.models.load_model('Disc_V1.h5')\n",
        "  return disc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pHp3bFD7IIf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is defining a function called \"generate_real_fake_wrong_samples()\".\n",
        "The purpose of this function is to generate samples of 3 different types: real, fake and wrong.\n",
        "\n",
        "It starts by generating \"fake\" samples. It initializes an empty array called \"fake_y\" with 4000 zero values.\n",
        "It also initializes empty lists called \"fake_features\" and \"fake_caps\".\n",
        "It then iterates through the \"fake_captions\" dictionary, each iteration, it takes the image feature and\n",
        "caption and appends it to the \"fake_features\" and \"fake_caps\" lists respectively.\n",
        "\n",
        "Then it generates \"real\" samples. It initializes an empty array called \"real_y\" with 8000 one values.\n",
        "It also initializes empty lists called \"real_features\" and \"real_caps\".\n",
        "It then iterates through the \"train_caps\" dictionary, each iteration, it takes the image feature and 2\n",
        "random captions of the same image and appends it to the \"real_features\" and \"real_caps\" lists respectively.\n",
        "\n",
        "Then it generates \"wrong\" samples. It initializes an empty array called \"wrong_y\" with 4000 zero values.\n",
        "It also initializes empty lists called \"wrong_features\" and \"wrong_caps\".\n",
        "It then iterates through the \"train_caps\" dictionary, each iteration, it takes the image feature and a\n",
        "random caption of another image and appends it to the \"wrong_features\" and \"wrong_caps\" lists respectively.\n",
        "\n",
        "Finally, it concatenates the real, fake and wrong features, captions and labels (real_y, fake_y, wrong_y)\n",
        "into 3 arrays called \"disc_features\", \"disc_caps\" and \"disc_y\" and returns them.\n",
        "\"\"\"\n",
        "def generate_real_fake_wrong_samples():\n",
        "    # fake captions\n",
        "    fake_y = np.zeros((4000,))\n",
        "    fake_features = []\n",
        "    fake_caps = []\n",
        "    for url, caption in fake_captions.items():\n",
        "      feature = features[url]\n",
        "      fake_features.append(feature)\n",
        "      fake_caps.append(caption)\n",
        "\n",
        "    # real captions\n",
        "    real_y = np.ones((8000,))\n",
        "    real_features = []\n",
        "    real_caps = []\n",
        "    for img_id in train_caps:\n",
        "      img_url = img_id_to_url(img_id)\n",
        "      feature = features[img_url]\n",
        "      real_features.append(feature)\n",
        "      real_features.append(feature)\n",
        "      real_cap1, real_cap2 = random.sample(train_caps[img_id], 2)\n",
        "      real_caps.append(real_cap1)\n",
        "      real_caps.append(real_cap2)\n",
        "\n",
        "\n",
        "    wrong_y = np.zeros((4000,))\n",
        "    wrong_features = []\n",
        "    wrong_caps = []\n",
        "    for img_id in train_caps:\n",
        "      img_url = img_id_to_url(img_id)\n",
        "      feature = features[img_url]\n",
        "      wrong_features.append(feature)\n",
        "\n",
        "      # Finding another image\n",
        "      second_feature_sorted = False\n",
        "      while not second_feature_sorted:\n",
        "        second_id = random.choice(list(train_caps.keys()))\n",
        "        if img_id != second_id:\n",
        "          second_feature_sorted = True\n",
        "\n",
        "      wrong_caps.append(random.choice(train_caps[second_id]))\n",
        "\n",
        "    disc_features = real_features + fake_features + wrong_features\n",
        "    disc_caps = real_caps + fake_caps + wrong_caps\n",
        "    disc_y = np.concatenate((real_y, fake_y, wrong_y), axis = 0)\n",
        "    return disc_features, disc_caps, disc_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbUbPAFsLeee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYZKhAzZD1aA"
      },
      "outputs": [],
      "source": [
        "disc_features, disc_caps, disc_y = generate_real_fake_wrong_samples()\n",
        "disc_caps = np_disc_caps[:16000]\n",
        "np_disc_features = np.array(disc_features)\n",
        "np_disc_features = np_disc_features[:16000, :]\n",
        "np_disc_caps = np.array(disc_caps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iprp4ya_Leee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIbLNMyjJeVO"
      },
      "outputs": [],
      "source": [
        "disc_sequences = tokenizer.texts_to_sequences(np_disc_caps)\n",
        "disc_padded = pad_sequences(disc_sequences, maxlen=max_length)\n",
        "# np_disc_y = disc_y.reshape(12000,1)\n",
        "# disc_padded.shape\n",
        "# np_disc_features.shape\n",
        "# np_disc_caps.shape\n",
        "# disc.summary()\n",
        "print(np_disc_features.shape, disc_padded.shape, disc_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhOO6gy5GJ5T"
      },
      "outputs": [],
      "source": [
        "disc.compile(loss = 'binary_crossentropy',\n",
        "             optimizer = 'adam',\n",
        "             metrics = ['accuracy'])\n",
        "# np_disc_features.reshape([12000,1,2048]).shape\n",
        "disc.fit(x=[np_disc_features.reshape([16000,1,2048]), disc_padded], y=disc_y.reshape(16000,1), verbose=1, epochs=5, batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHWpVuJewtl4"
      },
      "outputs": [],
      "source": [
        "model_Disc_name = 'Disc_V1_coco' + '.h5'\n",
        "model_Disc_path = os.path.join(project_path, model_Disc_name)\n",
        "#only first time\n",
        "# disc.save(model_Disc_path )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwZC3R9z7sH-"
      },
      "source": [
        "## Training GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-MWbE3yAvdv"
      },
      "outputs": [],
      "source": [
        "gen = final_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYNJuDO0Rahv"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code defines a function named \"post_zero_to_pre\" that takes in an array as an input.\n",
        "The function first creates an empty list called \"non_zero\". It then iterates through each\n",
        "element in the input array, checks if the element is not equal to 0, and if it is not, it\n",
        "appends it to the \"non_zero\" list.\n",
        "\n",
        "After that, it creates an array of zeroes with the length of the difference between the\n",
        "length of the input array and the length of the \"non_zero\" list. Finally, it concatenates\n",
        "(combines) the array of zeroes with the \"non_zero\" list, and returns the resulting concatenated array.\n",
        "The function's purpose is to move all non zero elements of the array to the front of the array, and\n",
        "pad the remaining with zeroes.\n",
        "\"\"\"\n",
        "def post_zero_to_pre(array):\n",
        "  non_zero = []\n",
        "  the_len = len(array)\n",
        "  for num in array:\n",
        "    if num != 0:\n",
        "      non_zero.append(num)\n",
        "  # np_non_zero = np.array(non_zero)\n",
        "  pre_z = np.zeros(the_len - len(non_zero), dtype=np.int32)\n",
        "  return np.concatenate((pre_z, np.array(non_zero)), axis =0)\n",
        "\n",
        "# post_zero_to_pre([1,2,3, 0,0,0,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cUV3U4wPF2m"
      },
      "source": [
        "### Sampling and Creating Batches Of Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jzvoSbvyW6P"
      },
      "outputs": [],
      "source": [
        "t = [2, 12, 111, 5, 2, 74, 4]\n",
        "tt = []\n",
        "for i in range(1, len(t)+1):\n",
        "    curr_s = np.concatenate(([3]+t[:i], np.zeros((max_length - len(t[:i])-1))), axis=0)\n",
        "    tt.append(curr_s)\n",
        "    print(len(curr_s))\n",
        "print(tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj3pMa-bNHfl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The code is generating a caption for an image using reinforcement learning. It does so by creating an\n",
        "environment for the caption generation process, sampling words to add to the caption, and then adjusting\n",
        "the rewards based on the quality of the generated caption.\n",
        "\n",
        "It starts by initializing some variables and creating an environment object, called ICEnv, which takes\n",
        "the image, the maximum length of the caption, the number of words, and tokenizer as its inputs.\n",
        "\n",
        "Then, a while loop is used to generate the caption word by word. In each iteration, the current state\n",
        "of the caption is passed to a pre-trained model, which predicts the probability of each word in the\n",
        "vocabulary being the next word in the caption. Then, a word is sampled from this probability distribution and added to the caption. The environment is then updated with the new word and the process continues until the maximum length of the caption is reached or the 'endseq' token is generated.\n",
        "\n",
        "The function also returns the final reward, the state of the environment after each word, the actions\n",
        "taken, and the final generated caption.\n",
        "\"\"\"\n",
        "def sample_caption_rl(tokenizer, image, img_id, max_length, gmodel, num_words,\\\n",
        "                      with_render=True,with_img=True):\n",
        "  # Creating env\n",
        "  env = ICEnv(image= image, img_id= img_id,\n",
        "            max_len=max_length, num_words=num_words,\n",
        "            startseq = tokenizer.word_index['startseq'],\n",
        "            endseq = tokenizer.word_index['endseq'], tokenizer= tokenizer)\n",
        "  # Sampling\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  score = 0\n",
        "  if with_render:\n",
        "    env.render(with_image = with_img)\n",
        "  actions = []\n",
        "  caption = 'startseq'\n",
        "  total_states = []\n",
        "  while not done:\n",
        "      # env.render()\n",
        "      pre_zero_seq = post_zero_to_pre(env.state['words'])\n",
        "\n",
        "      word_probs = gmodel.predict([np.array(env.state['image']).reshape((1,2048)),\n",
        "                                  np.array([pre_zero_seq])],verbose=0)\n",
        "\n",
        "      dist = tfp.distributions.Categorical(probs=word_probs, dtype=tf.float32)\n",
        "      tf_predicted_index = dist.sample()\n",
        "      predicted_index = np.array(tf_predicted_index)\n",
        "\n",
        "      new_word = tokenizer.sequences_to_texts([predicted_index])[0]\n",
        "      caption += ' ' + new_word\n",
        "\n",
        "      action = tokenizer.word_index[new_word]\n",
        "      actions.append(action)\n",
        "\n",
        "      # print(env.state['words'])\n",
        "\n",
        "      n_state, reward, done, info = env.step(action)\n",
        "\n",
        "\n",
        "  for i in range(1, len(actions)+1):\n",
        "    curr_state = np.concatenate(([3]+actions[:i-1], np.zeros((max_length - len(actions[:i-1])-1), dtype=np.int32)), axis=0)\n",
        "    total_states.append({'image':env.state['image'],\n",
        "                         'words':curr_state})\n",
        "\n",
        "  return reward, total_states, actions, caption\n",
        "\n",
        "\n",
        "sample_caption_rl(tokenizer, features['000000338532.jpg'], 338532,\\\n",
        "                  max_length, gen, num_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeH8Gbo08Dh9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code creates a batch of fake captions for an image dataset. The batch size is specified by the user\n",
        "and the captions are generated by the sample_caption_rl function. The function starts by initializing an\n",
        "empty dictionary called fake_batch and three empty lists called total_actions, total_rewards, and total_urls.\n",
        "\n",
        "\n",
        "The code enters a while loop, where it continues to iterate until the length of the fake_batch dictionary\n",
        "reaches the desired batch size. Inside the while loop, the function chooses a random image id from the\n",
        "train_caps dictionary using the random.choice method. Then it gets the image url using the img_id_to_url\n",
        "function. Next, it gets the feature of the image and reshapes it to a 1x2048 numpy array.\n",
        "\n",
        "The sample_caption_rl function is called with the tokenizer, feature, image id, max_length, gen, num_words,\n",
        "with_render and with_img parameters. This function generates a caption for the image and returns the reward,\n",
        "states, actions and the caption. The actions, rewards, and states are appended to the total_actions,\n",
        "total_rewards and total_states list respectively. The image url is added to the total_urls list. Finally,\n",
        "the caption is added to the fake_batch dictionary with the image url as the key.\n",
        "\n",
        "Once the while loop completes, the function returns the fake_batch, total_actions, total_rewards,\n",
        "total_urls, and total_states. These can be used for further processing or training.\n",
        "\"\"\"\n",
        "def create_fake_captions_batch(batch_size):\n",
        "  fake_batch = {}\n",
        "  total_actions = []\n",
        "  total_rewards = []\n",
        "  total_urls = []\n",
        "  total_states = []\n",
        "  while len(fake_batch) < batch_size:\n",
        "    img_id = random.choice(list(train_caps.keys()))\n",
        "    img_url = img_id_to_url(img_id)\n",
        "    feature = features[img_url].reshape((1,2048))\n",
        "\n",
        "    rewards, states, actions, caption = sample_caption_rl(tokenizer, feature,\\\n",
        "                                                          img_id, max_length,\\\n",
        "                                gen, num_words, with_render=False,\\\n",
        "                                 with_img=False)\n",
        "    total_actions.append(actions)\n",
        "    total_rewards.append(rewards)\n",
        "    fake_batch[img_url] = caption\n",
        "    total_urls.append(img_url)\n",
        "    total_states.append(states)\n",
        "  return fake_batch, total_actions, total_rewards, total_urls, total_states\n",
        "\n",
        "# create_fake_captions_batch(4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ_HV63KPSy2"
      },
      "source": [
        "### Calculating Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHMSkWpTCgw_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code defines a function called calculate_p that takes in five arguments:\n",
        "\n",
        "img_cap_pairs: a dictionary where the keys are image URLs and the values are captions\n",
        "disc: a discriminator model\n",
        "tokenizer: an object used to convert captions to numerical sequences\n",
        "max_length: the maximum length of captions\n",
        "batch_size: the number of images in the batch\n",
        "The function first takes the image URLs from the img_cap_pairs dictionary and uses them to retrieve the\n",
        "corresponding image features. These image features are then stored in a list called img_features.\n",
        "\n",
        "Next, the captions from the img_cap_pairs dictionary are stored in a list called captions. The tokenizer is\n",
        "used to convert the captions to numerical sequences, which are then padded to the max_length using the\n",
        "pad_sequences function. These padded sequences are then stored in a numpy array called np_padded.\n",
        "\n",
        "The disc model is then used to predict the probability that each image-caption pair is real.\n",
        "The image features, np_features, and the padded caption sequences, np_padded, are passed to the\n",
        "model as input. The predictions are stored in a numpy array called probs which is then flattened and returned.\n",
        "\n",
        "\"\"\"\n",
        "def calculate_p(img_cap_pairs, disc, tokenizer, max_length, batch_size):\n",
        "  img_urls = list(img_cap_pairs.keys())\n",
        "  img_features = []\n",
        "  for url in img_urls:\n",
        "    if url in t_features:\n",
        "      feature = t_features[url]\n",
        "    else:\n",
        "      feature = features[url]\n",
        "    img_features.append(feature)\n",
        "\n",
        "  captions = list(img_cap_pairs.values())\n",
        "\n",
        "  # print(np.array(img_features).shape, np.array(captions).shape)\n",
        "  # x=[np_disc_features.reshape([16000,1,2048]), disc_padded], y=disc_y.reshape(16000,1)\n",
        "  np_features = np.array(img_features).reshape(batch_size, 1, 2048)\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(captions)\n",
        "  padded = pad_sequences(sequences, maxlen=max_length)\n",
        "  np_padded = np.array(padded)\n",
        "\n",
        "  probs = disc.predict([np_features, np_padded])\n",
        "\n",
        "  return probs.flatten()\n",
        "# calculate_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG8juUjvFuuX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is defining a function called \"calculate_s\" that takes in a dictionary of image-caption pairs\n",
        "(img_cap_pairs) as its input.\n",
        "\n",
        "The function creates an empty list called \"s_values\", and then loops through each image URL (url) in the\n",
        "input dictionary. For each image URL, it uses a function called \"compute_bleu\" to calculate a score\n",
        "(bleu_score) for the associated caption (img_cap_pairs[url]). It then appends this score to the\n",
        "\"s_values\" list.\n",
        "\n",
        "Finally, the function returns the \"s_values\" list, which contains the scores for each caption in the input\n",
        "dictionary.\n",
        "\"\"\"\n",
        "def calculate_s(img_cap_pairs):\n",
        "  s_values = []\n",
        "  for url in img_cap_pairs:\n",
        "    bleu_score = compute_bleu(url, img_cap_pairs[url], False)\n",
        "    s_values.append(bleu_score)\n",
        "  return s_values\n",
        "\n",
        "# calculate_s(fake_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6yK4lexIpQ0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is defining a function called \"calculate_r\" that takes in three inputs:\n",
        "\"p_values\", \"s_values\", and \"lambda_val\". The function uses numpy to create numpy\n",
        "arrays from the input lists \"p_values\" and \"s_values\". The function then multiplies\n",
        "the array of \"p_values\" by \"lambda_val\" and the array of \"s_values\" by \"(1-lambda_val)\".\n",
        "It then adds the two resulting arrays together, element-wise and return the sum.\n",
        "The result is the rewards for each image caption pair.\n",
        "\"\"\"\n",
        "def calculate_r(p_values, s_values, lambda_val):\n",
        "  np_p = np.array(p_values)\n",
        "  np_s = np.array(s_values)\n",
        "\n",
        "  return lambda_val*np_p + (1-lambda_val)*np_s\n",
        "\n",
        "calculate_r([1,2,3,4], [0,0,0,0], 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of7BOf1IPWxx"
      },
      "source": [
        "### Updatin Generator Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKd1DNxMPf04"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is calculating a reward for a generated caption, using a combination of two different metrics:\n",
        "BLEU score and a discriminator score.\n",
        "\n",
        "First, it takes an image url and generates a caption using a pre-trained generator model (gmodel) and a\n",
        "tokenizer. The generated caption is then passed to the function compute_bleu() to calculate the BLEU score,\n",
        "which measures the similarity of the generated caption to a set of reference captions.\n",
        "\n",
        "Next, the generated caption and image url are passed to the function calculate_p() which uses a discriminator\n",
        "model (dmodel) and tokenizer to predict the probability of the generated caption being real.\n",
        "\n",
        "Finally, the code calculates the final reward as a linear combination of the BLEU score and the discriminator\n",
        "score, where the weight of each component is controlled by the parameter lambda_val. The final reward is\n",
        "returned.\n",
        "\n",
        "\"\"\"\n",
        "def calculate_greedy_decoding_reward(img_url, tokenizer, gmodel, max_len,\\\n",
        "                                     for_training=True, dmodel=None,\\\n",
        "                                     lambda_val=0):\n",
        "  if for_training:\n",
        "    feature = features[img_url]\n",
        "  else:\n",
        "    feature = t_features[img_url]\n",
        "  picture = np.array(feature).reshape((1,2048))\n",
        "  caption = generate_caption(tokenizer, picture, max_len, gmodel)\n",
        "  # print(caption)\n",
        "\n",
        "  bleu_score = compute_bleu(img_url, caption, not for_training)\n",
        "  disc_score = calculate_p({img_url: caption}, dmodel,\\\n",
        "                           tokenizer,max_len, 1)\n",
        "\n",
        "  # print('bleu: ',bleu_score, 'p: ', disc_score,\\\n",
        "  #       'reward: ',bleu_score*(1-lambda_val) + disc_score*(lambda_val),\\\n",
        "  #       'lambda: ', lambda_val)\n",
        "  return bleu_score*(1-lambda_val) + disc_score*(lambda_val)\n",
        "\n",
        "# calculate_greedy_decoding_reward('000000540466.jpg', tokenizer, gen, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMAQ5St9O0mD"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code defines a function called \"calculate_loss\", which calculates the loss for a given action, reward,\n",
        "and set of probabilities.\n",
        "The function takes a few input arguments.\n",
        "greedy_reward: a float representing the reward of the caption generated by a greedy decoding algorithm\n",
        "The function first create a categorical distribution with probs. Then it calculates the log probability of the\n",
        "action.\n",
        "It then calculates the loss as the negative of the product of log_prob and the difference between the reward\n",
        "and greedy_reward.\n",
        "It then checks if the loss is NaN, and if it is, it prints the minimum and maximum values of the probs array.\n",
        "It returns the loss value calculated.\n",
        "\"\"\"\n",
        "def calculate_loss(gmodel, action, reward, probs, img_url, tokenizer, max_len, greedy_reward):\n",
        "    dist = tfp.distributions.Categorical(probs=probs, dtype=tf.float32)\n",
        "    log_prob = dist.log_prob(action)\n",
        "    loss = -log_prob*(reward - greedy_reward)\n",
        "    # print('reward, greedy reward:', reward, greedy_reward)\n",
        "    # print('max prob and action are:', np.argmax(probs), action)\n",
        "    if math.isnan(loss):\n",
        "      print('loss reached nan')\n",
        "      print(np.min(probs), np.max(probs))\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm-ihZguJ1nb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is defining a function called update_generator that takes in several parameters such as a list of\n",
        "rewards (r_values), a batch of data (batch), a list of actions (actions), the current states of the generator\n",
        "model (states), the generator model (gmodel), a list of image URLs (urls), the maximum length of captions\n",
        "(max_len), the discriminator model (dmodel), and a lambda value (lambda_val).\n",
        "\n",
        "The function then defines an Adam optimizer and initializes a counter variable. It then loops through the\n",
        "zip of the states, rewards, actions, and image URLs, and for each iteration, it calculates the \"greedy\n",
        "reward\" which is the reward of the caption generated by the generator model, by calling the function\n",
        "calculate_greedy_decoding_reward. Then it loops through the current actions and states and inside the\n",
        "loop, it uses TensorFlow's GradientTape to record the forward pass of the generator model. It converts\n",
        "the state's 'words' to a pre-zero format and passes the state's 'image' and pre_zero_seq to the generator\n",
        "model, and get the word probabilities from the model. It then checks if the minimum value of word_probs is\n",
        "zero or NaN, if so it will break the loop. If not, it calculates the loss using the function calculate_loss\n",
        "with the action, reward, word_probs, img_url, tokenizer, max_len, and greedy_reward as arguments. It then\n",
        "calculates the gradients using GradientTape and applies the gradients to the generator model using the\n",
        "optimizer. Finally, it returns the updated generator model.\n",
        "\"\"\"\n",
        "def update_generator(r_values, batch, actions, states, gmodel, urls, max_len,\\\n",
        "                     dmodel, lambda_val):\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005, clipvalue=.5)\n",
        "  counter = 0\n",
        "\n",
        "  for current_states, reward, current_actions, img_url in zip(states, r_values, actions, urls):\n",
        "    counter += 1\n",
        "    # if counter %2 == 0:\n",
        "      # print(f'{100*(counter/len(urls))}%')\n",
        "\n",
        "    greedy_reward = calculate_greedy_decoding_reward(img_url, tokenizer,\\\n",
        "                                                       gmodel, max_len, True,\\\n",
        "                                                       dmodel, lambda_val)\n",
        "\n",
        "    for action, state in zip(current_actions, current_states):\n",
        "\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "\n",
        "        pre_zero_seq = post_zero_to_pre(state['words'])\n",
        "        word_probs = gmodel([np.array(state['image']).reshape((1,2048)),\n",
        "                                  np.array([pre_zero_seq])], training = True)\n",
        "\n",
        "        if np.min(word_probs) == 0:\n",
        "          # word_probs += 1.1499141e-24\n",
        "          # print('here')\n",
        "          break\n",
        "        if math.isnan(np.min(word_probs)):\n",
        "          print('model reached nan')\n",
        "          return\n",
        "\n",
        "        loss = calculate_loss(gmodel, action, reward, word_probs, img_url, tokenizer, max_len, greedy_reward)\n",
        "\n",
        "\n",
        "      grads = tape.gradient(loss, gmodel.trainable_variables)\n",
        "\n",
        "      optimizer.apply_gradients(zip(grads, gmodel.trainable_variables))\n",
        "  return gmodel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmdm6oo4T_Ei"
      },
      "outputs": [],
      "source": [
        "model_final_name = 'final_model_coco_V5.h5'\n",
        "model_final_path = os.path.join(project_path, model_final_name)\n",
        "gen = tf.keras.models.load_model(model_final_path)\n",
        "tpic = np.array(features['000000540466.jpg']).reshape((1,2048))\n",
        "# generate_caption(tokenizer, tpic, max_length, gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POunWOFzOZHj"
      },
      "source": [
        "### Updating Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES2enUl7SR4a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is generating fake captions by selecting a random set of images from a dataset and passing them\n",
        "through a language model (gmodel). The function takes as input the language model, a tokenizer, a maximum\n",
        "length for the captions, and the number of samples (n_samples) to generate. It first selects n_samples\n",
        "randomly from the dataset, then for each selected image, it passes the image through the language model with\n",
        "the generate_caption() function to generate a caption. The generated captions are then stored in a dictionary\n",
        "(gen_captions) with the image URL as the key and the generated caption as the value. The function returns\n",
        "this dictionary of generated captions.\n",
        "\"\"\"\n",
        "def generate_fake_captions(gmodel, tokenizer, max_len, n_samples):\n",
        "  gen_captions = {}\n",
        "  all_feature_keys = list(features.keys())\n",
        "  random_img_urls = random.sample(all_feature_keys, n_samples)\n",
        "  for img_url in random_img_urls:\n",
        "    image = features[img_url].reshape((1,2048))\n",
        "\n",
        "    gen_cap = generate_caption(tokenizer, image, max_len, gmodel)\n",
        "\n",
        "    gen_captions[img_url] = gen_cap\n",
        "\n",
        "  return gen_captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNwK-YqYNv16"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is generating a set of fake, real and wrong captions with their corresponding image features.\n",
        "It starts by initializing some variables, such as the number of samples for each type of caption (real, fake\n",
        "and wrong),\n",
        "and an array of all image ids in the training set.\n",
        "\n",
        "It then generates fake captions using the generate_fake_captions function and collects the corresponding\n",
        "image feature and caption for each generated fake caption.\n",
        "It then selects a random set of real captions from the training set, along with their corresponding image\n",
        "features,\n",
        "It also selects a random set of wrong captions, which are captions that are not associated with the\n",
        "corresponding image feature,\n",
        "and also collects their corresponding image feature.\n",
        "Finally, it concatenates all the real, fake, and wrong image features, captions and labels,\n",
        "pad the captions and return the concatenated image features, padded captions and labels as a tuple.\n",
        "\"\"\"\n",
        "def generate_real_fake_wrong_samples2(n_samples, gmodel):\n",
        "    all_img_ids = list(train_caps.keys())\n",
        "    f_samples = int(n_samples/4)\n",
        "    r_samples = int(n_samples/4)\n",
        "    w_samples = int(n_samples/4)\n",
        "\n",
        "    # fake captions\n",
        "    fake_y = np.zeros((f_samples,))\n",
        "    fake_features = []\n",
        "    fake_caps = []\n",
        "    generated_captions = generate_fake_captions(gmodel, tokenizer, max_length, f_samples)\n",
        "    for url, caption in generated_captions.items():\n",
        "      feature = features[url]\n",
        "      fake_features.append(feature)\n",
        "      fake_caps.append(caption)\n",
        "\n",
        "    # real captions\n",
        "    real_y = np.ones((2*r_samples,))\n",
        "    real_features = []\n",
        "    real_caps = []\n",
        "    random_r_ids = random.sample(all_img_ids, r_samples)\n",
        "    for img_id in random_r_ids:\n",
        "      img_url = img_id_to_url(img_id)\n",
        "      feature = features[img_url]\n",
        "      real_features.append(feature)\n",
        "      real_features.append(feature)\n",
        "      real_cap1, real_cap2 = random.sample(train_caps[img_id], 2)\n",
        "      real_caps.append(real_cap1)\n",
        "      real_caps.append(real_cap2)\n",
        "\n",
        "\n",
        "\n",
        "    # wrong captions\n",
        "    wrong_y = np.zeros((w_samples,))\n",
        "    wrong_features = []\n",
        "    wrong_caps = []\n",
        "    random_w_ids = random.sample(all_img_ids, w_samples)\n",
        "    for img_id in random_w_ids:\n",
        "      img_url = img_id_to_url(img_id)\n",
        "      feature = features[img_url]\n",
        "      wrong_features.append(feature)\n",
        "\n",
        "      # Finding another image\n",
        "      second_feature_sorted = False\n",
        "      while not second_feature_sorted:\n",
        "        second_id = random.choice(list(train_caps.keys()))\n",
        "        if img_id != second_id:\n",
        "          second_feature_sorted = True\n",
        "\n",
        "      wrong_caps.append(random.choice(train_caps[second_id]))\n",
        "\n",
        "    disc_features = real_features + fake_features + wrong_features\n",
        "    disc_caps = real_caps + fake_caps + wrong_caps\n",
        "    disc_y = np.concatenate((real_y, fake_y, wrong_y), axis = 0)\n",
        "\n",
        "    # added\n",
        "    np_disc_features = np.array(disc_features)\n",
        "    np_disc_caps = np.array(disc_caps)\n",
        "\n",
        "    disc_sequences = tokenizer.texts_to_sequences(np_disc_caps)\n",
        "    disc_padded = pad_sequences(disc_sequences, maxlen=max_length)\n",
        "    print(np_disc_features.shape, disc_padded.shape, disc_y.shape)\n",
        "\n",
        "    return np_disc_features, disc_padded, disc_y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yHI_0UiOfgi"
      },
      "source": [
        "## Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfQNzuxM7xi0"
      },
      "outputs": [],
      "source": [
        "converged = False\n",
        "mini_batch = 8\n",
        "disc_data_batch = 128\n",
        "disc_training_batch = 8\n",
        "lambda_val = 0.2\n",
        "disc_epochs = 4\n",
        "n_epochs = 25\n",
        "\"\"\"\n",
        "This code is training a generative model called \"gen\" and a discriminative model called \"disc\" using a\n",
        "technique called reinforcement learning. The code runs for a certain number of epochs (n_epochs) and in\n",
        "each epoch it performs the following steps:\n",
        "\n",
        "Generate a batch of fake captions (fake_batch) using the generator model and some additional inputs\n",
        "(actions, rewards, urls, states) by calling the function create_fake_captions_batch(mini_batch)\n",
        "Calculate the probability (p_values) of the fake captions being real using the discriminator model.\n",
        "Calculate the relevance (s_values) of the fake captions using some additional inputs (urls)\n",
        "Calculate the rewards (r_values) of the fake captions by combining the probability and relevance values with\n",
        "a parameter lambda_val\n",
        "Update the generator model using the rewards, fake captions, actions, states, urls and discriminator model\n",
        "by calling the function update_generator(r_values, fake_batch, actions, states, gen, urls, max_length, disc,\n",
        "lambda_val)\n",
        "Generate a batch of real, fake and wrong captions (disc_features_batch, disc_padded_batch, disc_y_batch)\n",
        "using the generator model and the function generate_real_fake_wrong_samples2(disc_data_batch, gen)\n",
        "Update the discriminator model using the generated captions and their labels (real, fake, wrong) by\n",
        "calling the function disc\n",
        "\"\"\"\n",
        "for i in range(n_epochs):\n",
        "  print(f'Epoch {i+1}/{n_epochs}')\n",
        "  # g part\n",
        "  print('generating samples for generator ...')\n",
        "  fake_batch, actions, rewards, urls, states = create_fake_captions_batch(mini_batch)\n",
        "\n",
        "  p_values = calculate_p(fake_batch, disc, tokenizer, max_length, mini_batch)\n",
        "  s_values = calculate_s(fake_batch)\n",
        "  r_values = calculate_r(p_values, s_values, lambda_val)\n",
        "  print('updating generator ...')\n",
        "  gen = update_generator(r_values, fake_batch, actions, states, gen, urls,\\\n",
        "                         max_length, disc, lambda_val)\n",
        "  # d part\n",
        "  # break\n",
        "  print('generating samples for discriminator ...')\n",
        "  disc_features_batch, disc_padded_batch, disc_y_batch = generate_real_fake_wrong_samples2(disc_data_batch, gen)\n",
        "  print('updating discriminator ...')\n",
        "  disc.fit(x=[disc_features_batch.reshape([disc_data_batch,1,2048]),\\\n",
        "              disc_padded_batch],y=disc_y_batch.reshape(disc_data_batch,1),\\\n",
        "           verbose=1, epochs=disc_epochs,\n",
        "           batch_size = disc_training_batch)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztZmL5CFWjTP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code defines a function named 't' that takes in two parameters, 'number' and 'second'\n",
        "(with a default value of 20 if no value is passed in for 'second'). The function performs the following\n",
        "operations:\n",
        "\n",
        "It creates a variable 'total_range' which is a range object from 1 to 1000.\n",
        "It creates a variable 'indexes' which is the first 'number' elements of the 'total_range' variable.\n",
        "It initializes a variable 'sum' to 0.\n",
        "It iterates over the 'indexes' variable using a tqdm progress bar. For each iteration, the following is\n",
        "done:\n",
        "a. It gets the image by indexing into the 't_features' variable using the current iteration variable.\n",
        "b. It reshapes the image to a 1x2048 shape.\n",
        "c. It generates a caption for the image using a function 'generate_caption' passing tokenizer,image,\n",
        "max_length and gen as parameter.\n",
        "d. It adds the bleu score for the caption and image using 'compute_bleu' function passing current iteration\n",
        "variable and generated caption as parameter.\n",
        "It prints out the average bleu score for all the captions generated.\n",
        "\"\"\"\n",
        "def t(number, second = 20):\n",
        "  total_range = range(1,1000)\n",
        "  # indexes = random.sample(total_range, number)\n",
        "  indexes = total_range[0:number]\n",
        "  sum = 0.\n",
        "  for img_index in tqdm.tqdm(indexes):\n",
        "    pic = list(t_features.keys())[img_index]\n",
        "    image = t_features[pic].reshape((1,2048))\n",
        "    cap = generate_caption(tokenizer, image, max_length, gen)\n",
        "    sum += compute_bleu(pic, cap)\n",
        "\n",
        "  print(sum/number)\n",
        "\"\"\"\n",
        "This code defines a function named \"t2\" that takes in two arguments: \"img_index\" and \"with_print\".\n",
        "The function retrieves the image corresponding to the provided \"img_index\" from the dictionary \"t_features\"\n",
        "and reshapes it to a 1x2048 array.\n",
        "Then it uses the \"generate_caption\" function to generate a caption for the image.\n",
        "Then, it prints the generated caption and the BLEU score if \"with_print\" is True.\n",
        "Then it returns a tuple that contains the BLEU score, a greedy decoding reward, the generated caption and\n",
        "the image.\n",
        "The greedy decoding reward is calculated by calling the function \"calculate_greedy_decoding_reward\" using\n",
        "the generated caption, tokenizer, generator, max_length, disc, lambda_val.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def t2(img_index, with_print = True):\n",
        "  pic = list(t_features.keys())[img_index]\n",
        "  image = t_features[pic].reshape((1,2048))\n",
        "  cap = generate_caption(tokenizer, image, max_length, gen)\n",
        "  if with_print:\n",
        "    print(cap)\n",
        "    print(compute_bleu(pic, cap))\n",
        "  return compute_bleu(pic, cap),\\\n",
        "   calculate_greedy_decoding_reward(pic, tokenizer, gen,\\\n",
        "                                                        max_length, False,\\\n",
        "                                                        disc, lambda_val), cap\\\n",
        "                                                        , pic\n",
        "\n",
        "t(100)\n",
        "t(20)\n",
        "t2(0)\n",
        "t2(10)\n",
        "t2(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkBs5o4ysAau"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code defines a function best_caps() that selects the \"best\" captions for a given number of images.\n",
        "The function starts by initializing variables to keep track of the best captions, rewards, and BLEU scores.\n",
        "It then enters a while loop where it repeatedly calls the t2() function, passing in a different image index\n",
        "each time. The t2() function generates a caption for the image and calculates the BLEU score, reward, and\n",
        "the caption and image url. If the BLEU score is greater than 0.8, the caption, reward, and url are added\n",
        "to the best_caps, rewards, and bleus dictionaries respectively. The loop continues until the specified number\n",
        "of captions have been added to the dictionaries. The function then returns the best_caps, rewards, and bleus\n",
        "dictionaries.\n",
        "\"\"\"\n",
        "def best_caps(num = 5):\n",
        "  counter = 0\n",
        "  index = 0\n",
        "  best_caps = {}\n",
        "  rewards = {}\n",
        "  bleus = {}\n",
        "  while counter <num:\n",
        "    bleu, reward, caption, img_url = t2(index, False)\n",
        "    if bleu > 0.8:\n",
        "      best_caps[img_url] = caption\n",
        "      rewards[img_url] = reward\n",
        "      bleus[img_url] = bleu\n",
        "      counter += 1\n",
        "      print('item added')\n",
        "\n",
        "    index +=1\n",
        "\n",
        "  return best_caps, rewards, bleus\n",
        "\n",
        "bc, rs,bs = best_caps(10)\n",
        "print(bc, rs,bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC7nrOsavr1u"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code defines a function called 'show_selected_results' that takes in 3 inputs: 'caps', 'rewards', and\n",
        "'bleus'. It starts by creating a list of URLs (keys) from the 'caps' input, and then loops through each\n",
        "URL in the list. For each URL, it calls a function 'calculate_greedy_decoding_reward' with the URL, a\n",
        "tokenizer, a generator model, a max_length, a boolean value of False, a discriminator model, and a\n",
        "lambda_val as inputs. Then it loads an image from a directory path using the URL, shows the image,\n",
        "prints the caption, BLEU score, and reward associated with that URL from the 'caps', 'bleus', and\n",
        "'rewards' inputs respectively. It displays the selected results in the form of image, caption, BLEU score,\n",
        "and reward for the images with BLEU score greater than 0.8.\n",
        "\n",
        "\"\"\"\n",
        "def show_selected_results(caps, rewards, bleus):\n",
        "  urls = list(caps.keys())\n",
        "  for pic in urls:\n",
        "    print('url: ', pic)\n",
        "    calculate_greedy_decoding_reward(pic, tokenizer, gen,\\\n",
        "                                                        max_length, False,\\\n",
        "                                                        disc, lambda_val)\n",
        "\n",
        "    x = plt.imread(test_dir_path+ '/' + pic)\n",
        "    plt.imshow(x)\n",
        "    plt.show()\n",
        "    print('Caption: ', caps[pic])\n",
        "    print('BLEU score: ', bleus[pic], 'Reward: ', rewards[pic])\n",
        "\n",
        "show_selected_results(bc,rs,bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zfkfir2eLeej"
      },
      "outputs": [],
      "source": [
        "# cd /content/drive/MyDrive/Lab_work/Video_Caption/training_caption/ImageCaptioning-main/dataset_MSCOCO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyVCPGT9Leej"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT1hiLP_VRx1"
      },
      "outputs": [],
      "source": [
        "# !wget https://archive.org/download/MSCoco2014/train2014.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8NfEdyweeWx"
      },
      "outputs": [],
      "source": [
        "# !unzip /content/drive/MyDrive/Lab_work/Video_Caption/training_caption/ImageCaptioning-main/train2014.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7i8poy2Leej"
      },
      "outputs": [],
      "source": [
        "# !wget https://archive.org/download/MSCoco2014/val2014.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1aVZXOSU-km"
      },
      "outputs": [],
      "source": [
        "# !unzip /content/drive/MyDrive/Lab_work/Video_Caption/training_caption/ImageCaptioning-main/dataset_MSCOCO/val2014.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUbQ5S2RYyfz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}